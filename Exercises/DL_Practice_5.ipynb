{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters optimisation\n",
    "## TD 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is here, you can download Food-3 or Food-3-big if you want more data (https://drive.google.com/drive/u/1/folders/1kMhMH5pi_jJNgwNPpy1epXsWCI4Hqsve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are essentially going to use the same `Food101` ([credit where it's due](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)) data, the same object `ImageDataset`, the same `DataLoader`.\n",
    "\n",
    "The code below is mainly a copy of the code from the previous TD, except that global variables are now defined separately and everything is wrapped in different functions. This is to make it easier to train the same model with different hyperparameters and architectures, etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those that can use their GPUs or are on the DCE, all the necessary `.to(device)` are already in the code.\n",
    "\n",
    "If, for some reason, you encounter this error: `OutOfMemoryError: CUDA out of memory.`, it means that your GPU does not have enough memory to run the model. You can try to reduce the batch size, or the number of neurons in the network, or the number of layers in the network, or the number of filters in the convolutional layers, etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pathlib\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "_ = torch.manual_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "print = partial(print, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Setup device-agnostic code\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE} device\")\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 2e-2\n",
    "\n",
    "# Number of epochs\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Number of classes\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# Image size\n",
    "IMAGE_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_and_dataloaders(\n",
    "    batch_size: int = 4\n",
    ") -> tuple[\n",
    "    DataLoader, \n",
    "    DataLoader\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load the training and test datasets into data loaders.\n",
    "    \"\"\"\n",
    "    data_dir = pathlib.Path(\".\")\n",
    "    train_dir = data_dir / \"Food-3\" / \"train\"\n",
    "    test_dir = data_dir / \"Food-3\" / \"test\"\n",
    "\n",
    "    data_transform_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=(IMAGE_SIZE, IMAGE_SIZE)),  # Resize the images to 128x128\n",
    "            transforms.ToTensor(),  # Convert the images to tensors\n",
    "            transforms.RandomHorizontalFlip(p=0.5),  # Flip the images horizontally with probability 0.5\n",
    "        ]\n",
    "    )\n",
    "    data_transform_test = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=(IMAGE_SIZE, IMAGE_SIZE)),  # Resize the images to 128x128\n",
    "            transforms.ToTensor(),  # Convert the images to tensors\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_data = datasets.ImageFolder(\n",
    "        root=str(train_dir),  # target folder of images\n",
    "        transform=data_transform_train,  # transforms to perform on data (images)\n",
    "        target_transform=None  # transforms to perform on labels (if necessary)\n",
    "    ) \n",
    "\n",
    "    test_data = datasets.ImageFolder(\n",
    "        root=str(test_dir),\n",
    "        transform=data_transform_test\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,  # how many samples per batch?\n",
    "        shuffle=True  # shuffle the data?\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    ) # don't usually need to shuffle testing data\n",
    "\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataloaders in global variables\n",
    "TRAIN_DATALOADER, TEST_DATALOADER = get_datasets_and_dataloaders(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we get the datasets? Did we lose them? No\n",
    "print(TRAIN_DATALOADER.dataset)\n",
    "print(\"---\")\n",
    "print(TEST_DATALOADER.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units: int = 200,\n",
    "        batch_norm: bool = False,\n",
    "        use_conv: bool = False\n",
    "    ):\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Why does it not work with `x = x.view(BATCH_SIZE, -1)`?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "MODEL: Net = Net(hidden_units=200, batch_norm=False, use_conv=False).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_our_model() -> float:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our untrained model\n",
    "print((f\"{100*test_our_model():.2f}%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get 34.31% accuracy on the testing set without training and with the default hyperparameters if you used the same seed. Close enough to 33% which is the expected accuracy for a random classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot some images like last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tensors and put them in the right dimensions for matplotlib\n",
    "my_pizza = TRAIN_DATALOADER.dataset[0][0]\n",
    "my_label = TRAIN_DATALOADER.dataset[0][1]\n",
    "print(my_pizza.shape)\n",
    "my_pizza_reshaped = my_pizza.permute(1, 2, 0)\n",
    "print(my_pizza_reshaped.shape)\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(my_pizza_reshaped)\n",
    "plt.axis(\"off\")\n",
    "plt.title(TRAIN_DATALOADER.dataset.classes[my_label], fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the training loop now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(loss_fn, optimizer) -> None:\n",
    "    \"\"\"\n",
    "    Train the model and modifies the trained model inplace.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_train(nn.CrossEntropyLoss(), torch.optim.SGD(MODEL.parameters(), lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((f\"{100*test_our_model():.2f}%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get 58.17% accuracy on the testing set without training and with the default hyperparameters if you used the same seed. And we definitely reached convergence (the loss is not decreasing that much anymore, and if you try to train for more epochs, you will see that the testing set accuracy will decrease). Note that by saying \"if you try to train for more epochs, you will see that the testing set accuracy will decrease\", we kind of cheated by using the testing set to infer an information about the number of epochs, we should instead use validation sets and cross validation techniques ... and we will (today)! No worries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could that help? How is it a trade off?\n",
    "```py\n",
    "class CachedImageFolder(datasets.ImageFolder):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.cache = {}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index not in self.cache:\n",
    "            self.cache[index] = super().__getitem__(index)\n",
    "        return self.cache[index]\n",
    "```\n",
    "\n",
    "We're losing something by doing that (unrelated to the trade-off mentioned above), what is it? How could we fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible for `train_loss` to decrease whilst `train_acc` decreases at the same time? Look at what happens between epochs 14 and 15 in this example (it's a real run, different seed though):\n",
    "```py\n",
    "epoch 14/15, train_loss = 1.01e-01, train_acc = 63.61%, time spent ...\n",
    "epoch 15/15, train_loss = 9.91e-02, train_acc = 63.50%, time spent ...\n",
    "```\n",
    "\n",
    "Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to improve this accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install the Optuna package (`pip install optuna`) and import it at the beginning of your script (no need if you're using the shared environment of the DCE, we installed it for you already). We should also import `KFold` from `sklearn.model_selection`. This is because we will use cross-validation to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First easy task is to decide how many neurons there should be in the hidden layer.\n",
    " \n",
    "We will do this together (optimising the number of hidden nuerons), and then you'll have to implement optimization of the learning rate*, the optimizer's choice on your own. We will also show you how to choose between a convolutional and dense network.\n",
    "\n",
    "\\**Careful! Small learning rates are not always better, especially if you do not change the number of epochs. You should try to find the best learning rate for the number of epochs you chose, one that is not too big for your computer to handle.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to define a new function that will be used as the objective function for Optuna's optimization. This function should take in the `trial` object from Optuna as an argument and use the `trial` object to define and sample the hyperparameters that you want to optimize. For example, you can use the `trial` object to sample a choice between a convolutional and dense network, and to sample the number of neurons for the chosen network. After training the model, we will need to return the final validation accuracy calculated with cross-validation* as the objective function value for Optuna to maximise.\n",
    "\n",
    "\\*We use cross-validation here (5-fold) because we want to use the testing set as little as possible. We will use the testing set only once, at the end, to get the final accuracy of the best model. But, cross-validation greatly increases the time required to run the algorithms, so we won't always use cross-validation to optimize hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will need to call the `optuna.create_study()` function to create a new study, and use the `study.optimize()` function to run the optimization, passing the objective function that we defined earlier.\n",
    "\n",
    "You can find more information about how to use Optuna in the [Optuna documentation](https://optuna.readthedocs.io/en/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, timeout=1200, n_trials=15) \n",
    "# - timeout = 1200 -> stops after 20 minutes;\n",
    "# - n_trials = 5 -> tries 5 different values for the hyperparameter.\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"\\n\")\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hyperparam_values = [t.params['hidden_units'] for t in complete_trials]\n",
    "accuracies = [t.value for t in complete_trials]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(hyperparam_values, accuracies)\n",
    "plt.title('Hyperparameter Optimization Results')\n",
    "plt.xlabel('Value of hidden_units')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More neurons on the hidden layers is better (when considering the range 5-50). It makes sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few of you might have a problem: we've only allowed 15 trials, but `Optuna` tried twice the same trial (unlikely if you set a big range but likely if you only allowed the range [5; 8] for example. An example: https://i.imgur.com/DceMhuf.png). This is because `Optuna` doesn't check if it already has used the previous set of hyperparameters. To fix this, we can add the following code:\n",
    "\n",
    "```py\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    for previous_trial in trial.study.trials:\n",
    "        if previous_trial.state == TrialState.COMPLETE and trial.params == previous_trial.params:\n",
    "            print(f\"Duplicated trial: {trial.params}, return {previous_trial.value}\")\n",
    "            return previous_trial.value\n",
    "    ...\n",
    "...\n",
    "```\n",
    "\n",
    "And even setting n_trials to 5000, we won't have optuna running two \"experiments\" with the same hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add this and let's also optimize on the learning rate, the number of epochs, the use of batch normalisation layers, and the optimizer's choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add some manual pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.exceptions import TrialPruned\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, timeout=3600, n_trials=20) \n",
    "# - timeout = 3600 -> stops after 60 minutes;\n",
    "# - n_trials = 20 -> tries 20 different values for the hyperparameter.\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"--------------------\")\n",
    "print(\"\\n\")\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the whole model with the optimal hyperparameters that we found with Optuna. We will use the `study.best_params` attribute to get the best hyperparameters. You need to re-train on the whole training dataset!!! Otherwise, you will not get the best accuracy as you're leaving out some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our performance went up by more than 10%! (Only 20 trials of hyperparameter optimisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a61f982c8ae83496d3304782998ac96a47f5fcf9bfc174247e19a8162c5490e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
